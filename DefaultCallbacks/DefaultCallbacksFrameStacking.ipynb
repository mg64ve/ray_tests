{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc33e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from gym.envs.classic_control import CartPoleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81afc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessCartPole(CartPoleEnv):\n",
    "    \"\"\"Partially observable variant of the CartPole gym environment.\n",
    "\n",
    "    https://github.com/openai/gym/blob/master/gym/envs/classic_control/\n",
    "    cartpole.py\n",
    "\n",
    "    We delete the x- and angular velocity components of the state, so that it\n",
    "    can only be solved by a memory enhanced model (policy).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Fix our observation-space (remove 2 velocity components).\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                self.theta_threshold_radians * 2,\n",
    "            ],\n",
    "            dtype=np.float32)\n",
    "\n",
    "        self.observation_space = Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_obs, reward, done, info = super().step(action)\n",
    "        # next_obs is [x-pos, x-veloc, angle, angle-veloc]\n",
    "        return np.array([next_obs[0], next_obs[2]]), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        init_obs = super().reset()\n",
    "        # init_obs is [x-pos, x-veloc, angle, angle-veloc]\n",
    "        return np.array([init_obs[0], init_obs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194af51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 17:55:18,527\tWARNING deprecation.py:45 -- DeprecationWarning: `ray.rllib.utils.window_stat.WindowStat` has been deprecated. Use `ray.rllib.utils.metrics.window_stat.WindowStat` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models.torch.misc import SlimFC\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.policy.view_requirement import ViewRequirement\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_utils import one_hot as torch_one_hot\n",
    "from ray.rllib.utils.typing import ModelConfigDict, TensorType\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb205329",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch, nn = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0fe845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFrameStackingCartPoleModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"A simple FC model that takes the last n observations as input.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 obs_space,\n",
    "                 action_space,\n",
    "                 num_outputs,\n",
    "                 model_config,\n",
    "                 name,\n",
    "                 num_frames=3):\n",
    "        nn.Module.__init__(self)\n",
    "        super(TorchFrameStackingCartPoleModel, self).__init__(\n",
    "            obs_space, action_space, None, model_config, name)\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # Construct actual (very simple) FC model.\n",
    "        assert len(obs_space.shape) == 1\n",
    "        in_size = self.num_frames * (obs_space.shape[0] + action_space.n + 1)\n",
    "        self.layer1 = SlimFC(\n",
    "            in_size=in_size, out_size=256, activation_fn=\"relu\")\n",
    "        self.layer2 = SlimFC(in_size=256, out_size=256, activation_fn=\"relu\")\n",
    "        self.out = SlimFC(\n",
    "            in_size=256, out_size=self.num_outputs, activation_fn=\"linear\")\n",
    "        self.values = SlimFC(in_size=256, out_size=1, activation_fn=\"linear\")\n",
    "\n",
    "        self._last_value = None\n",
    "\n",
    "        self.view_requirements[\"prev_n_obs\"] = ViewRequirement(\n",
    "            data_col=\"obs\",\n",
    "            shift=\"-{}:0\".format(num_frames - 1),\n",
    "            space=obs_space)\n",
    "        self.view_requirements[\"prev_n_rewards\"] = ViewRequirement(\n",
    "            data_col=\"rewards\", shift=\"-{}:-1\".format(self.num_frames))\n",
    "        self.view_requirements[\"prev_n_actions\"] = ViewRequirement(\n",
    "            data_col=\"actions\",\n",
    "            shift=\"-{}:-1\".format(self.num_frames),\n",
    "            space=self.action_space)\n",
    "\n",
    "    def forward(self, input_dict, states, seq_lens):\n",
    "        obs = input_dict[\"prev_n_obs\"]\n",
    "        obs = torch.reshape(obs, [-1, self.obs_space.shape[0] * self.num_frames])\n",
    "        rewards = torch.reshape(input_dict[\"prev_n_rewards\"],\n",
    "                                [-1, self.num_frames])\n",
    "        actions = torch_one_hot(input_dict[\"prev_n_actions\"],\n",
    "                                self.action_space)\n",
    "        actions = torch.reshape(actions, [-1, self.num_frames * actions.shape[-1]])\n",
    "        input_ = torch.cat([obs, actions, rewards], dim=-1)\n",
    "        features = self.layer1(input_)\n",
    "        features = self.layer2(features)\n",
    "        out = self.out(features)\n",
    "        self._last_value = self.values(features)\n",
    "        return out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.squeeze(self._last_value, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf8f898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation import Episode, RolloutWorker\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_step(self, *, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        policies: Dict[str, Policy], episode: Episode,\n",
    "                        env_index: int, **kwargs):\n",
    "        # Make sure this episode is ongoing.\n",
    "        assert episode.length > 0, \\\n",
    "            \"ERROR: `on_episode_step()` callback should not be called right \" \\\n",
    "            \"after env reset!\"\n",
    "        print(episode.input_dict[\"prev_n_obs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d56d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 17:55:27,561\tWARNING services.py:1838 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.18.0.3',\n",
       " 'raylet_ip_address': '172.18.0.3',\n",
       " 'redis_address': '172.18.0.3:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-12-14_17-55-25_995835_32/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-12-14_17-55-25_995835_32/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-12-14_17-55-25_995835_32',\n",
       " 'metrics_export_port': 51534,\n",
       " 'node_id': '4cfa206f85296778205593c3a2a7826cd686ff59bc5a6bbd760a4418'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=0 or None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea4cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"frame_stack_model\", TorchFrameStackingCartPoleModel)\n",
    "register_env(\"StatelessPendulum\", lambda _: StatelessCartPole())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c34ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StatelessCartPole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699869a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01925686,  0.02920437], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 20\n",
    "config = {\n",
    "    \"env\": 'StatelessPendulum',\n",
    "    \"gamma\": 0.9,\n",
    "    \"num_gpus\": 0,\n",
    "    \"num_workers\": 0,\n",
    "    \"num_envs_per_worker\": 20,\n",
    "    \"callbacks\": MyCallbacks,\n",
    "    \"entropy_coeff\": 0.001,\n",
    "    \"num_sgd_iter\": 5,\n",
    "    \"vf_loss_coeff\": 1e-5,\n",
    "    \"model\": {\n",
    "        \"vf_share_layers\": True,\n",
    "        \"custom_model\": \"frame_stack_model\",\n",
    "        \"custom_model_config\": {\n",
    "            \"num_frames\": num_frames,\n",
    "        },\n",
    "    },\n",
    "    \"framework\": 'torch',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e8a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 10,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 300.,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5364222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 17:55:43,243\tINFO logger.py:605 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2021-12-14 17:55:43,245\tWARNING callback.py:114 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
      "2021-12-14 17:55:43,669\tERROR syncer.py:111 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=166)\u001b[0m 2021-12-14 17:55:45,020\tWARNING deprecation.py:45 -- DeprecationWarning: `ray.rllib.utils.window_stat.WindowStat` has been deprecated. Use `ray.rllib.utils.metrics.window_stat.WindowStat` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=166)\u001b[0m 2021-12-14 17:55:45,419\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=166)\u001b[0m 2021-12-14 17:55:45,419\tINFO trainer.py:719 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-14 17:55:46 (running for 00:00:02.86)<br>Memory usage on this node: 2.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/46.22 GiB heap, 0.0/9.31 GiB objects<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=166)\u001b[0m 2021-12-14 17:55:46,217\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_base_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=166)\u001b[0m 2021-12-14 17:55:46,242\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-12-14 17:55:46,887\tERROR trial_runner.py:958 -- Trial PPO_StatelessPendulum_acd02_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/tune/trial_runner.py\", line 924, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py\", line 783, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/worker.py\", line 1712, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::PPO.train()\u001b[39m (pid=166, ip=172.18.0.3, repr=PPO)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "    result = self.step()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 861, in step\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 843, in step\n",
      "    result = self.step_attempt()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 896, in step_attempt\n",
      "    step_results = next(self.train_exec_impl)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py\", line 76, in sampler\n",
      "    yield workers.local_worker().sample()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 757, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 265, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 633, in _env_runner\n",
      "    _process_observations(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 918, in _process_observations\n",
      "    callbacks.on_episode_step(\n",
      "  File \"/tmp/ipykernel_32/1556064808.py\", line 14, in on_episode_step\n",
      "AttributeError: 'Episode' object has no attribute 'input_dict'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trial PPO_StatelessPendulum_acd02_00000 errored with parameters={'env': 'StatelessPendulum', 'gamma': 0.9, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'callbacks': <class '__main__.MyCallbacks'>, 'entropy_coeff': 0.001, 'num_sgd_iter': 5, 'vf_loss_coeff': 1e-05, 'model': {'vf_share_layers': True, 'custom_model': 'frame_stack_model', 'custom_model_config': {'num_frames': 20}}, 'framework': 'torch'}. Error file: /home/condauser/ray_results/PPO/PPO_StatelessPendulum_acd02_00000_0_2021-12-14_17-55-43/error.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-14 17:55:46 (running for 00:00:03.55)<br>Memory usage on this node: 2.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/46.22 GiB heap, 0.0/9.31 GiB objects<br>Result logdir: /home/condauser/ray_results/PPO<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StatelessPendulum_acd02_00000</td><td>ERROR   </td><td>172.18.0.3:166</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                       </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StatelessPendulum_acd02_00000</td><td style=\"text-align: right;\">           1</td><td>/home/condauser/ray_results/PPO/PPO_StatelessPendulum_acd02_00000_0_2021-12-14_17-55-43/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_StatelessPendulum_acd02_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/64694177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = tune.run('PPO', \n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    checkpoint_at_end=True)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_StatelessPendulum_acd02_00000])"
     ]
    }
   ],
   "source": [
    "results = tune.run('PPO', \n",
    "                   config=config, \n",
    "                   stop=stop, \n",
    "                   verbose=2,\n",
    "                   checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cd8776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 17:50:25,786\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2021-12-10 17:50:25,798\tINFO trainable.py:467 -- Restored on 172.18.0.3 from checkpoint: /home/condauser/ray_results/PPO/PPO_StatelessPendulum_2fef5_00000_0_2021-12-10_17-49-48/checkpoint_000025/checkpoint-25\n",
      "2021-12-10 17:50:25,800\tINFO trainable.py:475 -- Current state after restoring: {'_iteration': 25, '_timesteps_total': 0, '_time_total': 33.76933407783508, '_episodes_total': 1487}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward=10.0\n",
      "Episode reward=10.0\n",
      "Episode reward=9.0\n",
      "Episode reward=10.0\n",
      "Episode reward=10.0\n",
      "Episode reward=10.0\n",
      "Episode reward=10.0\n",
      "Episode reward=10.0\n",
      "Episode reward=8.0\n",
      "Episode reward=10.0\n"
     ]
    }
   ],
   "source": [
    "checkpoints = results.get_trial_checkpoints_paths(\n",
    "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.restore(checkpoint_path)\n",
    "\n",
    "# Inference loop.\n",
    "env = StatelessCartPole()\n",
    "\n",
    "# Run manual inference loop for n episodes.\n",
    "for _ in range(10):\n",
    "    episode_reward = 0.0\n",
    "    reward = 0.0\n",
    "    action = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        # Create a dummy action using the same observation n times,\n",
    "        # as well as dummy prev-n-actions and prev-n-rewards.\n",
    "        action, state, logits = trainer.compute_single_action(\n",
    "            input_dict={\n",
    "                \"obs\": obs,\n",
    "                \"prev_n_obs\": np.stack([obs for _ in range(num_frames)]),\n",
    "                \"prev_n_actions\": np.stack([0 for _ in range(num_frames)]),\n",
    "                \"prev_n_rewards\": np.stack(\n",
    "                    [1.0 for _ in range(num_frames)]),\n",
    "            },\n",
    "            full_fetch=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(f\"Episode reward={episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
